# -*- coding: utf-8 -*-
"""YOUTUBE_CHATBOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13GFzgovU5BPlodK7UTqCQW75aPHfwj1v
"""

import os
# os.environ["OPENAI_API_KEY"]="paste the openai_api key here"
os.environ["OPENAI_API_KEY"]="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"

# !pip install -q youtube-transcript-api langchain-community langchain-openai \
                # faiss-cpu tiktoken python-dotenv

# !pip install -U langchain-huggingface

from youtube_transcript_api import YouTubeTranscriptApi,TranscriptsDisabled
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings,ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate

"""#indexing"""

# indexing step-data_loading
video_id = "uiHeQg-DFFE"
try:
  # if you don"t care which language ,this returns the 'best' one
    transcript_list = YouTubeTranscriptApi.get_transcript(video_id,languages=['hi','en'])
    # print(transcript_list)
    # flatten it to plain text
    transcript=" ".join(chunk['text'] for chunk in transcript_list)
    print(transcript)
except TranscriptsDisabled:
  print('No Caption available for this video')

print(len(transcript_list))

# text_splitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1000,
    chunk_overlap = 200
)
chunks = splitter.create_documents([transcript])

len(chunks)

# creating the vector_stores
# embeddings = OpenAIEmbeddings(model='text-embedding-3-small')
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
vector_store =FAISS.from_documents(chunks,embedding_model)

vector_store.index_to_docstore_id

vector_store.get_by_ids(["paste the ids provided in above code lines response"])

"""#step-2 Retrieval"""

retriever = vector_store.as_retriever(search_type='similarity',search_kwargs={'k':4})

retriever

retriever.invoke("what is langraph and also its advantage over langchains?")

"""#Step-3 //Agumentation"""

llm = ChatOpenAI(
    openai_api_base="https://api.groq.com/openai/v1",
    openai_api_key=os.environ["OPENAI_API_KEY"],
    model_name="llama3-70b-8192",  # or "llama3-70b", "gemma-7b"
    temperature=0.7,
)
# result=llm.invoke('what is black hole?')
# print(result.content)
# llm = ChatOpenAI(
#     model='gpt-3.5-turbo',
#     temperature = 0.5
# )

prompt = PromptTemplate(
    template ='''
    You are a helpful assistant.
    Answer ONLY from the provided transcripts context.
    If the context is insufficient ,just say you dont know.

    {context}
    Question ; {question}''',
    input_variables=['context','question']
)

question = "what is langraph and also its advantage over langchains?"
retrieved_docs =retriever.invoke(question)

context_text = "\n\n".join(docs.page_content for docs in retrieved_docs)

final_prompt = prompt.invoke({'context':context_text ,'question':question})

final_prompt

"""#step-4 GENERATION"""

answer = llm.invoke(final_prompt)
print(answer.content)

"""#Building a chain"""

from langchain_core.runnables import RunnableParallel,RunnablePassthrough,RunnableLambda
from langchain_core.output_parsers import StrOutputParser

def format_docs(retrieved_docs):
  context_text ='\n\n'.join(docs.page_content for docs in retrieved_docs)
  return context_text

parallel_chain = RunnableParallel({
    'context':retriever | RunnableLambda(format_docs),
    'question':RunnablePassthrough()
})

parser =StrOutputParser()

main_chain =parallel_chain | prompt | llm | parser



print(main_chain.invoke("what all the steps to be taken to crack companies?"))

